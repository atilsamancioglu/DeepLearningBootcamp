{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeece9cf-4885-4796-b99f-619bc60ee846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7f5ef05-69ba-4c5d-a160-12c19f5bdf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 32, 32]) -> [batch_size, color_channels, height, width]\n",
      "Single image shape: torch.Size([3, 32, 32]) -> [color_channels, height, width]\n",
      "Single image pixel values:\n",
      "tensor([[[ 1.9269e+00,  1.4873e+00,  9.0072e-01,  ...,  4.1759e-02,\n",
      "          -2.5158e-01,  8.5986e-01],\n",
      "         [-1.3847e+00, -8.7124e-01, -2.2337e-01,  ...,  1.8446e+00,\n",
      "          -1.1845e+00,  1.3835e+00],\n",
      "         [ 1.4451e+00,  8.5641e-01,  2.2181e+00,  ..., -8.2777e-01,\n",
      "           1.3347e+00,  4.8354e-01],\n",
      "         ...,\n",
      "         [ 5.1823e-02, -3.2848e-01, -2.2472e+00,  ...,  1.4557e+00,\n",
      "          -3.4610e-01, -2.6338e-01],\n",
      "         [-4.4770e-01, -7.2882e-01, -1.6066e-01,  ...,  5.4047e-01,\n",
      "           4.3507e-01, -2.2717e+00],\n",
      "         [-1.3386e-01, -5.8557e-02,  1.2574e-01,  ...,  1.1085e+00,\n",
      "           5.5442e-01,  1.5818e+00]],\n",
      "\n",
      "        [[-1.2248e+00,  9.6289e-01, -1.5785e+00,  ...,  7.8247e-01,\n",
      "          -6.4659e-02, -2.2984e-04],\n",
      "         [ 6.8309e-01,  1.0637e-01,  3.5032e-01,  ..., -1.0381e+00,\n",
      "          -1.0130e-01, -9.2718e-01],\n",
      "         [ 2.3484e-01,  8.8615e-02, -3.4769e-01,  ..., -1.1049e+00,\n",
      "          -7.9095e-01, -2.1609e-01],\n",
      "         ...,\n",
      "         [-8.3326e-01, -9.5268e-01, -3.6367e-01,  ..., -3.4518e-01,\n",
      "          -7.2878e-01, -1.4958e+00],\n",
      "         [ 7.3510e-01, -2.6932e-01,  4.5086e-01,  ...,  4.1639e-01,\n",
      "           6.7471e-01,  2.2499e-01],\n",
      "         [-9.5563e-01, -7.7798e-01,  6.9351e-01,  ..., -3.3411e-02,\n",
      "          -8.2765e-01, -3.5242e-01]],\n",
      "\n",
      "        [[-6.0023e-01, -5.7975e-02,  2.9749e-01,  ..., -3.3021e-01,\n",
      "          -7.3943e-01, -1.5103e+00],\n",
      "         [ 1.7838e-01, -1.7935e-01, -5.5837e-01,  ...,  4.9892e-01,\n",
      "          -1.0029e+00,  6.7861e-02],\n",
      "         [-5.3456e-01, -1.4052e+00,  1.8912e+00,  ...,  1.7921e+00,\n",
      "          -7.6727e-01, -1.3080e+00],\n",
      "         ...,\n",
      "         [ 4.7382e-01,  3.1056e-02, -1.4894e-01,  ...,  1.7253e+00,\n",
      "           1.2699e-01, -8.8103e-01],\n",
      "         [-6.3806e-01,  5.3368e-01,  1.6807e-01,  ..., -5.0325e-01,\n",
      "           2.1168e+00,  1.2190e+00],\n",
      "         [-7.8533e-01,  1.0901e+00, -6.6455e-02,  ..., -2.3891e+00,\n",
      "           7.1780e-01, -1.5831e+00]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# a sample tensor representing our cifar10 data (batch of 32)\n",
    "images = torch.randn(size=(32, 3, 32, 32)) # [batch_size, color_channels, height, width]\n",
    "test_image = images[0] \n",
    "print(f\"Image batch shape: {images.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Single image shape: {test_image.shape} -> [color_channels, height, width]\") \n",
    "print(f\"Single image pixel values:\\n{test_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88888ab-06ec-4ef2-ba15-dbcaccc33a1f",
   "metadata": {},
   "source": [
    "https://poloclub.github.io/cnn-explainer/\n",
    "\n",
    "in_channels (int) - Number of channels in the input image.\n",
    "\n",
    "out_channels (int) - Number of channels produced by the convolution.\n",
    "\n",
    "kernel_size (int or tuple) - Size of the convolving kernel/filter.\n",
    "\n",
    "stride (int or tuple, optional) - How big of a step the convolving kernel takes at a time. Default: 1.\n",
    "\n",
    "padding (int, tuple, str) - Padding added to all four sides of input. Default: 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e93966f-fae4-4b62-83ff-306a53482a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1667e+00,  1.4325e-01,  1.6771e-02,  ...,  3.1925e-02,\n",
       "          -7.1931e-02,  3.7874e-01],\n",
       "         [ 9.5715e-01,  3.2787e-01,  2.6548e-01,  ..., -3.0249e-01,\n",
       "          -1.8824e-01, -3.8694e-01],\n",
       "         [ 2.7237e-02,  4.1275e-01,  1.0648e+00,  ..., -5.8316e-01,\n",
       "          -7.7496e-01,  7.0928e-01],\n",
       "         ...,\n",
       "         [-7.4959e-01, -5.7313e-01,  2.3699e-01,  ...,  8.5081e-01,\n",
       "           2.3852e-01, -4.1881e-01],\n",
       "         [-5.5988e-01,  9.8861e-02, -8.2409e-01,  ..., -4.4532e-02,\n",
       "           2.9962e-01, -2.9472e-02],\n",
       "         [ 1.2873e-01, -7.5458e-01, -4.0582e-01,  ..., -4.7212e-02,\n",
       "           1.3111e+00,  6.3414e-01]],\n",
       "\n",
       "        [[ 3.3298e-01,  3.3936e-01, -6.4400e-01,  ...,  1.4035e-01,\n",
       "          -6.6302e-01, -5.8043e-03],\n",
       "         [ 5.9086e-01, -1.1782e+00,  5.4756e-01,  ..., -5.9479e-01,\n",
       "          -5.3999e-01, -4.8962e-01],\n",
       "         [-1.6372e+00,  1.5448e-01,  2.5152e-01,  ..., -6.4175e-01,\n",
       "          -1.3647e-01, -3.4338e-01],\n",
       "         ...,\n",
       "         [-6.1613e-01, -1.8070e-01, -5.3585e-01,  ...,  6.0576e-02,\n",
       "          -7.8199e-01, -5.5082e-01],\n",
       "         [-3.3866e-01,  4.5890e-01,  3.7009e-01,  ..., -4.5851e-01,\n",
       "           4.1258e-03,  5.6639e-01],\n",
       "         [-4.5238e-01, -6.4545e-02, -6.6588e-01,  ..., -7.0030e-01,\n",
       "          -2.6113e-01, -1.6523e+00]],\n",
       "\n",
       "        [[-9.1101e-02, -8.7506e-01,  1.2345e+00,  ...,  6.2833e-01,\n",
       "           4.8458e-01,  3.1709e-02],\n",
       "         [ 1.2411e-01,  1.0399e+00, -3.5336e-01,  ..., -5.6362e-01,\n",
       "           5.1454e-01,  3.8916e-01],\n",
       "         [ 9.3818e-02, -5.8863e-01,  1.2193e+00,  ...,  8.0447e-01,\n",
       "          -9.2021e-01,  1.4810e+00],\n",
       "         ...,\n",
       "         [-5.9046e-01,  3.8891e-01,  7.4055e-02,  ..., -1.8687e-01,\n",
       "          -1.0062e-02, -1.4540e-01],\n",
       "         [ 2.6121e-01,  1.1619e-01, -4.6718e-01,  ...,  8.8121e-01,\n",
       "          -3.8153e-03,  1.6472e-01],\n",
       "         [ 2.9418e-01, -1.0674e-01,  6.5320e-01,  ..., -6.3434e-01,\n",
       "          -4.3987e-01,  8.7864e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 5.6378e-01,  3.2423e-01, -5.4748e-01,  ...,  1.7504e-01,\n",
       "           9.1699e-01, -2.7167e-01],\n",
       "         [ 9.6987e-02, -5.1847e-02,  1.2452e+00,  ...,  1.7428e-01,\n",
       "          -8.0471e-01,  5.3856e-01],\n",
       "         [-1.9392e-01,  5.0802e-01, -7.0252e-01,  ...,  9.4893e-02,\n",
       "           3.6089e-01, -7.0224e-01],\n",
       "         ...,\n",
       "         [-2.4431e-01,  4.6818e-01,  1.0942e-01,  ...,  2.0712e-01,\n",
       "           8.2194e-01,  5.5501e-01],\n",
       "         [-1.8439e-01, -5.3770e-01, -2.4583e-01,  ..., -1.1908e+00,\n",
       "           2.1718e-01,  4.7690e-01],\n",
       "         [-1.6204e-01,  6.2012e-01, -3.1261e-01,  ..., -3.9645e-01,\n",
       "           8.4727e-02,  1.7439e-01]],\n",
       "\n",
       "        [[-4.9959e-01, -9.2936e-02,  2.4273e-01,  ...,  4.9418e-01,\n",
       "          -3.3084e-01,  7.2970e-01],\n",
       "         [ 3.9005e-01, -4.6466e-01, -8.7945e-01,  ..., -3.1972e-01,\n",
       "          -1.1895e-01,  3.9109e-01],\n",
       "         [ 1.4823e-01, -3.2170e-01,  7.2396e-01,  ..., -2.9050e-01,\n",
       "           8.9830e-01, -3.0797e-01],\n",
       "         ...,\n",
       "         [-2.3516e-02, -3.1008e-01, -1.7348e-01,  ..., -9.3442e-01,\n",
       "          -9.4840e-01, -1.0398e+00],\n",
       "         [ 6.3253e-01,  1.0984e+00,  1.4620e+00,  ...,  1.1166e+00,\n",
       "           4.2612e-01,  8.5663e-01],\n",
       "         [-5.8787e-01, -6.1934e-04, -4.9534e-01,  ...,  5.7109e-03,\n",
       "          -1.0050e+00, -1.8119e+00]],\n",
       "\n",
       "        [[-9.6730e-01, -5.3148e-01,  1.8032e-01,  ..., -9.1236e-01,\n",
       "          -1.0970e-01, -6.8436e-01],\n",
       "         [-1.1101e+00, -1.9835e-01, -3.1921e-01,  ..., -5.5094e-01,\n",
       "           2.5635e-01, -5.5739e-01],\n",
       "         [-6.6276e-01,  6.4259e-01, -3.2899e-01,  ..., -3.1967e-02,\n",
       "           5.5094e-01,  2.5362e-01],\n",
       "         ...,\n",
       "         [ 9.9064e-02,  3.2111e-01, -7.3760e-01,  ..., -9.9800e-01,\n",
       "          -5.2452e-01,  2.5997e-01],\n",
       "         [-2.0308e-01,  3.3030e-01,  4.4987e-02,  ..., -7.4870e-01,\n",
       "          -3.1682e-01, -1.0840e-01],\n",
       "         [-8.5208e-02,  2.1000e-01, -1.0189e-01,  ..., -1.1047e-01,\n",
       "          -3.7954e-01,  5.1705e-02]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# let's see what happens when we create a convulution layer and put the image through it\n",
    "# (try changing any of the parameters and see what happens)\n",
    "conv_layer = nn.Conv2d(in_channels=3,\n",
    "                       out_channels=10,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=0) \n",
    "\n",
    "# Pass the data through the convolutional layer\n",
    "conv_layer(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54c1925-8cc5-4729-9971-19ed10c8722e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 30, 30])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer(test_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3f5bbc1-7360-43bf-be31-177ed728bebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "467764e4-687d-488c-a92c-3c07b53f86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see we begin to compress the image as we see in tinyvgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a9804b-c886-4255-9bda-ea4c8543fdcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 14, 14])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we change the conv layer parameters we might even get a different pixel count\n",
    "torch.manual_seed(42)\n",
    "# Create a new conv_layer with different values \n",
    "conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n",
    "                         out_channels=10,\n",
    "                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n",
    "                         stride=2,\n",
    "                         padding=0)\n",
    "\n",
    "# Pass single image through new conv_layer_2 \n",
    "conv_layer_2(test_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f13d5cfe-7dfe-4b7f-a113-3077307afaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see it brings it down to 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4cfc3-2deb-4bf2-8435-8c7150d3a169",
   "metadata": {},
   "source": [
    "\n",
    "# conv2d does not always decrease the width and height of image \n",
    "# it depends on *padding*, *stride* & *kernel_size* \n",
    "\n",
    "```python\n",
    "conv_layer = nn.Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=10,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=0  \n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conv2D çıktısının boyutu matematiksel olarak nasıl hesaplanır?**\n",
    "\n",
    "Her eksen için formül:\n",
    "\n",
    "output = (((W - K) + 2P) / S) + 1\n",
    "\n",
    "- **W**: input genişlik (32)\n",
    "- **K**: kernel size (3)\n",
    "- **P**: padding (0 veya 1 olabilir)\n",
    "- **S**: stride (1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dca047bf-3917-409d-91e1-905f7c1aa42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what happens in max poold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69436851-7371-4276-86ca-8b030c489d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image original shape: torch.Size([3, 32, 32])\n",
      "Test image with unsqueezed dimension: torch.Size([1, 3, 32, 32])\n",
      "Shape after going through conv_layer(): torch.Size([1, 10, 30, 30])\n",
      "Shape after going through conv_layer() and max_pool_layer(): torch.Size([1, 10, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "# Print out original image shape without and with unsqueezed dimension\n",
    "print(f\"Test image original shape: {test_image.shape}\")\n",
    "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n",
    "\n",
    "# Create a sample nn.MaxPoo2d() layer\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "# Pass data through just the conv_layer\n",
    "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
    "print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n",
    "\n",
    "# Pass data through the max pool layer\n",
    "test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n",
    "print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b413f39-123a-4929-91e7-7f51a1a468e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so in tinyvgg we have \n",
    "# block1 -> conv2d + relu + conv2d + relu + maxpoold\n",
    "# block2 -> conv2d + relu + conv2d + relu + maxpoold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e0dba2-ef44-4c04-bf53-fcd5501df677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([1, 3, 32, 32])\n",
      "After Conv1: torch.Size([1, 10, 32, 32])\n",
      "After Conv2: torch.Size([1, 10, 32, 32])\n",
      "After MaxPool1: torch.Size([1, 10, 16, 16])\n",
      "After Conv3: torch.Size([1, 10, 16, 16])\n",
      "After Conv4: torch.Size([1, 10, 16, 16])\n",
      "After MaxPool2: torch.Size([1, 10, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# so let's create a random tensor representing only 1 image (batch of 1)\n",
    "x = torch.randn(1, 3, 32, 32)   # [batch, channels, height, width]\n",
    "print(\"Input:\", x.shape)\n",
    "\n",
    "# 1. BLOCK\n",
    "conv1 = nn.Conv2d(3, 10, kernel_size=3, stride=1, padding=1)\n",
    "conv2 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "x = conv1(x)\n",
    "print(\"After Conv1:\", x.shape)\n",
    "\n",
    "x = conv2(x)\n",
    "print(\"After Conv2:\", x.shape)\n",
    "\n",
    "x = pool1(x)\n",
    "print(\"After MaxPool1:\", x.shape)\n",
    "\n",
    "# 2. BLOCK\n",
    "conv3 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "conv4 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "x = conv3(x)\n",
    "print(\"After Conv3:\", x.shape)\n",
    "\n",
    "x = conv4(x)\n",
    "print(\"After Conv4:\", x.shape)\n",
    "\n",
    "x = pool2(x)\n",
    "print(\"After MaxPool2:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7423a4a8-b98c-4115-8b6f-7bb5b0000697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see it goes down to 8*8, that's why we need to use 8*8 in classifier\n",
    "\n",
    "#        self.classifier = nn.Sequential(\n",
    "#            nn.Flatten(),\n",
    "#            nn.Linear(in_features=hidden_units*8*8, \n",
    "#                      out_features=output_shape)\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12a2e8-b9c1-4835-9764-4bb38fdfb2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
